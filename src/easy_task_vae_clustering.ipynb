{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE for Music Clustering - Easy Task\n",
    "## Unsupervised Learning Project: Variational Autoencoder for Music Genre Clustering\n",
    "\n",
    "This notebook implements:\n",
    "- Basic VAE for feature extraction from music data\n",
    "- K-Means clustering on latent features\n",
    "- Visualization using t-SNE and UMAP\n",
    "- Comparison with PCA + K-Means baseline\n",
    "- Evaluation using Silhouette Score and Calinski-Harabasz Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import tensorflow as tf\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q librosa\n",
    "!pip install -q umap-learn\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q matplotlib seaborn\n",
    "!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "# Visualization\n",
    "import umap\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading\n",
    "\n",
    "We'll use the GTZAN Genre Collection dataset which contains 1000 audio tracks across 10 genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GTZAN dataset\n",
    "!wget -q http://opihi.cs.uvic.ca/sound/genres.tar.gz\n",
    "!tar -xzf genres.tar.gz\n",
    "print(\"Dataset downloaded and extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATA_PATH = 'genres'\n",
    "GENRES = ['blues', 'classical', 'country', 'disco', 'hiphop', \n",
    "          'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "\n",
    "# Audio processing parameters\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 30  # seconds\n",
    "N_MFCC = 20    # Number of MFCC features\n",
    "HOP_LENGTH = 512\n",
    "\n",
    "print(f\"Genres: {GENRES}\")\n",
    "print(f\"Total genres: {len(GENRES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction (MFCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_features(file_path, n_mfcc=20, max_len=1293):\n",
    "    \"\"\"\n",
    "    Extract MFCC features from audio file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to audio file\n",
    "        n_mfcc: Number of MFCC coefficients\n",
    "        max_len: Maximum length of MFCC sequence\n",
    "    \n",
    "    Returns:\n",
    "        mfcc: MFCC features of shape (n_mfcc, max_len)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "        \n",
    "        # Extract MFCC features\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc, hop_length=HOP_LENGTH)\n",
    "        \n",
    "        # Pad or truncate to max_len\n",
    "        if mfcc.shape[1] < max_len:\n",
    "            mfcc = np.pad(mfcc, ((0, 0), (0, max_len - mfcc.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfcc = mfcc[:, :max_len]\n",
    "        \n",
    "        return mfcc\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process all audio files\n",
    "features_list = []\n",
    "labels_list = []\n",
    "file_paths = []\n",
    "\n",
    "print(\"Extracting MFCC features from audio files...\")\n",
    "for genre_idx, genre in enumerate(GENRES):\n",
    "    genre_path = os.path.join(DATA_PATH, genre)\n",
    "    if not os.path.exists(genre_path):\n",
    "        print(f\"Warning: {genre_path} not found, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    audio_files = [f for f in os.listdir(genre_path) if f.endswith('.wav')]\n",
    "    print(f\"Processing {genre}: {len(audio_files)} files\")\n",
    "    \n",
    "    for audio_file in audio_files:\n",
    "        file_path = os.path.join(genre_path, audio_file)\n",
    "        mfcc = extract_mfcc_features(file_path, n_mfcc=N_MFCC)\n",
    "        \n",
    "        if mfcc is not None:\n",
    "            features_list.append(mfcc)\n",
    "            labels_list.append(genre_idx)\n",
    "            file_paths.append(file_path)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(features_list)  # Shape: (n_samples, n_mfcc, time_steps)\n",
    "y = np.array(labels_list)     # Shape: (n_samples,)\n",
    "\n",
    "print(f\"\\nTotal samples loaded: {len(X)}\")\n",
    "print(f\"Feature shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "X_normalized = (X - np.mean(X, axis=0)) / (np.std(X, axis=0) + 1e-8)\n",
    "\n",
    "# Reshape for VAE input: (samples, height, width, channels)\n",
    "X_input = X_normalized[..., np.newaxis]  # Add channel dimension\n",
    "\n",
    "print(f\"Normalized input shape: {X_input.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Sample MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MFCC for one sample from each genre\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, genre in enumerate(GENRES):\n",
    "    # Find first sample of this genre\n",
    "    idx = np.where(y == i)[0][0]\n",
    "    \n",
    "    # Plot MFCC\n",
    "    img = librosa.display.specshow(X[idx], x_axis='time', ax=axes[i], cmap='coolwarm')\n",
    "    axes[i].set_title(f'{genre.capitalize()}')\n",
    "    axes[i].set_ylabel('MFCC Coefficients')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mfcc_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"MFCC visualization saved as 'mfcc_samples.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Basic VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Configuration\n",
    "LATENT_DIM = 32\n",
    "INPUT_SHAPE = X_input.shape[1:]  # (n_mfcc, time_steps, 1)\n",
    "\n",
    "print(f\"Input shape: {INPUT_SHAPE}\")\n",
    "print(f\"Latent dimension: {LATENT_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling layer for VAE\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Encoder\n",
    "def build_encoder(input_shape, latent_dim):\n",
    "    \"\"\"\n",
    "    Build encoder network.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of input (n_mfcc, time_steps, 1)\n",
    "        latent_dim: Dimension of latent space\n",
    "    \n",
    "    Returns:\n",
    "        encoder: Keras Model\n",
    "    \"\"\"\n",
    "    encoder_inputs = keras.Input(shape=input_shape, name='encoder_input')\n",
    "    \n",
    "    x = layers.Flatten()(encoder_inputs)\n",
    "    x = layers.Dense(512, activation='relu', name='encoder_dense_1')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation='relu', name='encoder_dense_2')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu', name='encoder_dense_3')(x)\n",
    "    \n",
    "    # Latent space parameters\n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    \n",
    "    # Sample from latent space\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    \n",
    "    encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    return encoder\n",
    "\n",
    "encoder = build_encoder(INPUT_SHAPE, LATENT_DIM)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Decoder\n",
    "def build_decoder(latent_dim, output_shape):\n",
    "    \"\"\"\n",
    "    Build decoder network.\n",
    "    \n",
    "    Args:\n",
    "        latent_dim: Dimension of latent space\n",
    "        output_shape: Shape of output (n_mfcc, time_steps, 1)\n",
    "    \n",
    "    Returns:\n",
    "        decoder: Keras Model\n",
    "    \"\"\"\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,), name='decoder_input')\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu', name='decoder_dense_1')(latent_inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation='relu', name='decoder_dense_2')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Dense(512, activation='relu', name='decoder_dense_3')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Output layer\n",
    "    output_dim = np.prod(output_shape)\n",
    "    x = layers.Dense(output_dim, activation='linear', name='decoder_output')(x)\n",
    "    \n",
    "    # Reshape to original shape\n",
    "    decoder_outputs = layers.Reshape(output_shape)(x)\n",
    "    \n",
    "    decoder = Model(latent_inputs, decoder_outputs, name='decoder')\n",
    "    return decoder\n",
    "\n",
    "decoder = build_decoder(LATENT_DIM, INPUT_SHAPE)\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build VAE\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            \n",
    "            # Reconstruction loss\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    tf.square(data - reconstruction), axis=(1, 2, 3)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # KL divergence loss\n",
    "            kl_loss = -0.5 * tf.reduce_mean(\n",
    "                tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "            )\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"total_loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction\n",
    "\n",
    "# Create VAE model\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3))\n",
    "\n",
    "print(\"VAE model created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='total_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='total_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Validation split: {VALIDATION_SPLIT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the VAE\n",
    "history = vae.fit(\n",
    "    X_input,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Total loss\n",
    "axes[0].plot(history.history['total_loss'], label='Total Loss')\n",
    "if 'val_total_loss' in history.history:\n",
    "    axes[0].plot(history.history['val_total_loss'], label='Val Total Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Total Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Reconstruction loss\n",
    "axes[1].plot(history.history['reconstruction_loss'], label='Reconstruction Loss')\n",
    "if 'val_reconstruction_loss' in history.history:\n",
    "    axes[1].plot(history.history['val_reconstruction_loss'], label='Val Reconstruction Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Reconstruction Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# KL loss\n",
    "axes[2].plot(history.history['kl_loss'], label='KL Loss')\n",
    "if 'val_kl_loss' in history.history:\n",
    "    axes[2].plot(history.history['val_kl_loss'], label='Val KL Loss')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Loss')\n",
    "axes[2].set_title('KL Divergence Loss')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history saved as 'training_history.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extract Latent Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract latent representations (z_mean) for all samples\n",
    "z_mean, z_log_var, z = encoder.predict(X_input, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Latent features shape: {z_mean.shape}\")\n",
    "print(f\"Using z_mean for clustering (more stable than sampled z)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clustering with K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (same as number of genres)\n",
    "N_CLUSTERS = len(GENRES)\n",
    "\n",
    "# K-Means on VAE latent features\n",
    "kmeans_vae = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=10)\n",
    "clusters_vae = kmeans_vae.fit_predict(z_mean)\n",
    "\n",
    "print(f\"VAE K-Means clustering completed!\")\n",
    "print(f\"Cluster distribution: {np.bincount(clusters_vae)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Baseline: PCA + K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten MFCC features for PCA\n",
    "X_flat = X_normalized.reshape(len(X_normalized), -1)\n",
    "\n",
    "# Apply PCA to reduce to same dimension as VAE latent space\n",
    "pca = PCA(n_components=LATENT_DIM, random_state=42)\n",
    "X_pca = pca.fit_transform(X_flat)\n",
    "\n",
    "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "print(f\"PCA features shape: {X_pca.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means on PCA features\n",
    "kmeans_pca = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=10)\n",
    "clusters_pca = kmeans_pca.fit_predict(X_pca)\n",
    "\n",
    "print(f\"PCA K-Means clustering completed!\")\n",
    "print(f\"Cluster distribution: {np.bincount(clusters_pca)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for VAE clustering\n",
    "silhouette_vae = silhouette_score(z_mean, clusters_vae)\n",
    "calinski_vae = calinski_harabasz_score(z_mean, clusters_vae)\n",
    "\n",
    "# Calculate metrics for PCA clustering\n",
    "silhouette_pca = silhouette_score(X_pca, clusters_pca)\n",
    "calinski_pca = calinski_harabasz_score(X_pca, clusters_pca)\n",
    "\n",
    "# Create results dataframe\n",
    "results = pd.DataFrame({\n",
    "    'Method': ['VAE + K-Means', 'PCA + K-Means'],\n",
    "    'Silhouette Score': [silhouette_vae, silhouette_pca],\n",
    "    'Calinski-Harabasz Index': [calinski_vae, calinski_pca]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLUSTERING EVALUATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save results\n",
    "results.to_csv('clustering_metrics.csv', index=False)\n",
    "print(\"\\nResults saved to 'clustering_metrics.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Silhouette Score\n",
    "axes[0].bar(['VAE + K-Means', 'PCA + K-Means'], \n",
    "            [silhouette_vae, silhouette_pca],\n",
    "            color=['#2ecc71', '#3498db'])\n",
    "axes[0].set_ylabel('Silhouette Score')\n",
    "axes[0].set_title('Silhouette Score Comparison\\n(Higher is Better)')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate([silhouette_vae, silhouette_pca]):\n",
    "    axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Calinski-Harabasz Index\n",
    "axes[1].bar(['VAE + K-Means', 'PCA + K-Means'], \n",
    "            [calinski_vae, calinski_pca],\n",
    "            color=['#2ecc71', '#3498db'])\n",
    "axes[1].set_ylabel('Calinski-Harabasz Index')\n",
    "axes[1].set_title('Calinski-Harabasz Index Comparison\\n(Higher is Better)')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate([calinski_vae, calinski_pca]):\n",
    "    axes[1].text(i, v + 2, f'{v:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Metrics comparison saved as 'metrics_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualization: t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE to VAE latent features\n",
    "print(\"Applying t-SNE to VAE latent features...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "z_tsne = tsne.fit_transform(z_mean)\n",
    "\n",
    "# Apply t-SNE to PCA features\n",
    "print(\"Applying t-SNE to PCA features...\")\n",
    "tsne_pca = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "pca_tsne = tsne_pca.fit_transform(X_pca)\n",
    "\n",
    "print(\"t-SNE completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize t-SNE results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "# Color maps\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "# VAE - Colored by true genre\n",
    "for i, genre in enumerate(GENRES):\n",
    "    mask = y == i\n",
    "    axes[0, 0].scatter(z_tsne[mask, 0], z_tsne[mask, 1], \n",
    "                      c=[colors[i]], label=genre, alpha=0.6, s=30)\n",
    "axes[0, 0].set_title('VAE Latent Space (t-SNE) - True Genres', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('t-SNE Component 1')\n",
    "axes[0, 0].set_ylabel('t-SNE Component 2')\n",
    "axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# VAE - Colored by predicted cluster\n",
    "for i in range(N_CLUSTERS):\n",
    "    mask = clusters_vae == i\n",
    "    axes[0, 1].scatter(z_tsne[mask, 0], z_tsne[mask, 1], \n",
    "                      c=[colors[i]], label=f'Cluster {i}', alpha=0.6, s=30)\n",
    "axes[0, 1].set_title('VAE Latent Space (t-SNE) - K-Means Clusters', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('t-SNE Component 1')\n",
    "axes[0, 1].set_ylabel('t-SNE Component 2')\n",
    "axes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# PCA - Colored by true genre\n",
    "for i, genre in enumerate(GENRES):\n",
    "    mask = y == i\n",
    "    axes[1, 0].scatter(pca_tsne[mask, 0], pca_tsne[mask, 1], \n",
    "                      c=[colors[i]], label=genre, alpha=0.6, s=30)\n",
    "axes[1, 0].set_title('PCA Features (t-SNE) - True Genres', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('t-SNE Component 1')\n",
    "axes[1, 0].set_ylabel('t-SNE Component 2')\n",
    "axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# PCA - Colored by predicted cluster\n",
    "for i in range(N_CLUSTERS):\n",
    "    mask = clusters_pca == i\n",
    "    axes[1, 1].scatter(pca_tsne[mask, 0], pca_tsne[mask, 1], \n",
    "                      c=[colors[i]], label=f'Cluster {i}', alpha=0.6, s=30)\n",
    "axes[1, 1].set_title('PCA Features (t-SNE) - K-Means Clusters', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('t-SNE Component 1')\n",
    "axes[1, 1].set_ylabel('t-SNE Component 2')\n",
    "axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tsne_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"t-SNE visualization saved as 'tsne_visualization.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualization: UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UMAP to VAE latent features\n",
    "print(\"Applying UMAP to VAE latent features...\")\n",
    "umap_reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "z_umap = umap_reducer.fit_transform(z_mean)\n",
    "\n",
    "# Apply UMAP to PCA features\n",
    "print(\"Applying UMAP to PCA features...\")\n",
    "umap_pca_reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "pca_umap = umap_pca_reducer.fit_transform(X_pca)\n",
    "\n",
    "print(\"UMAP completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize UMAP results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "# VAE - Colored by true genre\n",
    "for i, genre in enumerate(GENRES):\n",
    "    mask = y == i\n",
    "    axes[0, 0].scatter(z_umap[mask, 0], z_umap[mask, 1], \n",
    "                      c=[colors[i]], label=genre, alpha=0.6, s=30)\n",
    "axes[0, 0].set_title('VAE Latent Space (UMAP) - True Genres', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('UMAP Component 1')\n",
    "axes[0, 0].set_ylabel('UMAP Component 2')\n",
    "axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# VAE - Colored by predicted cluster\n",
    "for i in range(N_CLUSTERS):\n",
    "    mask = clusters_vae == i\n",
    "    axes[0, 1].scatter(z_umap[mask, 0], z_umap[mask, 1], \n",
    "                      c=[colors[i]], label=f'Cluster {i}', alpha=0.6, s=30)\n",
    "axes[0, 1].set_title('VAE Latent Space (UMAP) - K-Means Clusters', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('UMAP Component 1')\n",
    "axes[0, 1].set_ylabel('UMAP Component 2')\n",
    "axes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# PCA - Colored by true genre\n",
    "for i, genre in enumerate(GENRES):\n",
    "    mask = y == i\n",
    "    axes[1, 0].scatter(pca_umap[mask, 0], pca_umap[mask, 1], \n",
    "                      c=[colors[i]], label=genre, alpha=0.6, s=30)\n",
    "axes[1, 0].set_title('PCA Features (UMAP) - True Genres', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('UMAP Component 1')\n",
    "axes[1, 0].set_ylabel('UMAP Component 2')\n",
    "axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# PCA - Colored by predicted cluster\n",
    "for i in range(N_CLUSTERS):\n",
    "    mask = clusters_pca == i\n",
    "    axes[1, 1].scatter(pca_umap[mask, 0], pca_umap[mask, 1], \n",
    "                      c=[colors[i]], label=f'Cluster {i}', alpha=0.6, s=30)\n",
    "axes[1, 1].set_title('PCA Features (UMAP) - K-Means Clusters', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('UMAP Component 1')\n",
    "axes[1, 1].set_ylabel('UMAP Component 2')\n",
    "axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('umap_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"UMAP visualization saved as 'umap_visualization.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Reconstruction Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct samples from VAE\n",
    "n_samples = 5\n",
    "sample_indices = np.random.choice(len(X_input), n_samples, replace=False)\n",
    "\n",
    "reconstructions = vae.predict(X_input[sample_indices])\n",
    "\n",
    "# Visualize original vs reconstructed\n",
    "fig, axes = plt.subplots(n_samples, 2, figsize=(12, 3 * n_samples))\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    # Original\n",
    "    librosa.display.specshow(X_normalized[idx], x_axis='time', ax=axes[i, 0], cmap='coolwarm')\n",
    "    axes[i, 0].set_title(f'Original - {GENRES[y[idx]].capitalize()}')\n",
    "    axes[i, 0].set_ylabel('MFCC Coefficients')\n",
    "    \n",
    "    # Reconstructed\n",
    "    librosa.display.specshow(reconstructions[i, :, :, 0], x_axis='time', ax=axes[i, 1], cmap='coolwarm')\n",
    "    axes[i, 1].set_title(f'Reconstructed - {GENRES[y[idx]].capitalize()}')\n",
    "    axes[i, 1].set_ylabel('MFCC Coefficients')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vae_reconstructions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"VAE reconstructions saved as 'vae_reconstructions.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EASY TASK SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. Dataset:\")\n",
    "print(f\"   - Total samples: {len(X)}\")\n",
    "print(f\"   - Genres: {', '.join(GENRES)}\")\n",
    "print(f\"   - Feature extraction: MFCC ({N_MFCC} coefficients)\")\n",
    "\n",
    "print(\"\\n2. VAE Architecture:\")\n",
    "print(f\"   - Input shape: {INPUT_SHAPE}\")\n",
    "print(f\"   - Latent dimension: {LATENT_DIM}\")\n",
    "print(f\"   - Encoder: Dense layers (512 -> 256 -> 128 -> {LATENT_DIM})\")\n",
    "print(f\"   - Decoder: Dense layers ({LATENT_DIM} -> 128 -> 256 -> 512 -> Output)\")\n",
    "\n",
    "print(\"\\n3. Clustering Results:\")\n",
    "print(f\"   - Number of clusters: {N_CLUSTERS}\")\n",
    "print(f\"   - Algorithm: K-Means\")\n",
    "\n",
    "print(\"\\n4. Evaluation Metrics:\")\n",
    "print(f\"\\n   VAE + K-Means:\")\n",
    "print(f\"   - Silhouette Score: {silhouette_vae:.4f}\")\n",
    "print(f\"   - Calinski-Harabasz Index: {calinski_vae:.2f}\")\n",
    "\n",
    "print(f\"\\n   PCA + K-Means (Baseline):\")\n",
    "print(f\"   - Silhouette Score: {silhouette_pca:.4f}\")\n",
    "print(f\"   - Calinski-Harabasz Index: {calinski_pca:.2f}\")\n",
    "\n",
    "print(\"\\n5. Visualizations Generated:\")\n",
    "print(\"   ✓ MFCC samples\")\n",
    "print(\"   ✓ Training history\")\n",
    "print(\"   ✓ Metrics comparison\")\n",
    "print(\"   ✓ t-SNE visualization\")\n",
    "print(\"   ✓ UMAP visualization\")\n",
    "print(\"   ✓ VAE reconstructions\")\n",
    "\n",
    "print(\"\\n6. Key Findings:\")\n",
    "if silhouette_vae > silhouette_pca:\n",
    "    print(\"   ✓ VAE latent features produce better clusters than PCA (higher Silhouette Score)\")\n",
    "else:\n",
    "    print(\"   • PCA baseline performs better than VAE on Silhouette Score\")\n",
    "    \n",
    "if calinski_vae > calinski_pca:\n",
    "    print(\"   ✓ VAE shows better cluster separation (higher Calinski-Harabasz Index)\")\n",
    "else:\n",
    "    print(\"   • PCA baseline shows better cluster separation on Calinski-Harabasz Index\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EASY TASK COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save VAE model\n",
    "vae.save('vae_model.keras')\n",
    "encoder.save('encoder_model.keras')\n",
    "decoder.save('decoder_model.keras')\n",
    "\n",
    "# Save latent features and clusters\n",
    "np.save('vae_latent_features.npy', z_mean)\n",
    "np.save('pca_features.npy', X_pca)\n",
    "np.save('vae_clusters.npy', clusters_vae)\n",
    "np.save('pca_clusters.npy', clusters_pca)\n",
    "np.save('true_labels.npy', y)\n",
    "\n",
    "print(\"Models and data saved successfully!\")\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  - vae_model.keras\")\n",
    "print(\"  - encoder_model.keras\")\n",
    "print(\"  - decoder_model.keras\")\n",
    "print(\"  - vae_latent_features.npy\")\n",
    "print(\"  - pca_features.npy\")\n",
    "print(\"  - vae_clusters.npy\")\n",
    "print(\"  - pca_clusters.npy\")\n",
    "print(\"  - true_labels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Download Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file with all results\n",
    "import zipfile\n",
    "\n",
    "files_to_zip = [\n",
    "    'clustering_metrics.csv',\n",
    "    'mfcc_samples.png',\n",
    "    'training_history.png',\n",
    "    'metrics_comparison.png',\n",
    "    'tsne_visualization.png',\n",
    "    'umap_visualization.png',\n",
    "    'vae_reconstructions.png',\n",
    "    'vae_latent_features.npy',\n",
    "    'pca_features.npy',\n",
    "    'vae_clusters.npy',\n",
    "    'pca_clusters.npy',\n",
    "    'true_labels.npy'\n",
    "]\n",
    "\n",
    "with zipfile.ZipFile('easy_task_results.zip', 'w') as zipf:\n",
    "    for file in files_to_zip:\n",
    "        if os.path.exists(file):\n",
    "            zipf.write(file)\n",
    "\n",
    "print(\"Results packaged in 'easy_task_results.zip'\")\n",
    "print(\"\\nYou can download this file from Colab's file browser.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
